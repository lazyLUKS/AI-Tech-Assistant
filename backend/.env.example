# Backend Server Configuration
HOST_IP="127.0.0.1" # Use 0.0.0.0 to listen on all interfaces, or specific IP if needed
PORT=8000

# vLLM Configuration
MODEL_NAME="mistralai/Pixtral-12B-2409"
# Adjust based on your GPU capabilities, e.g., 4096, 8192
MAX_MODEL_LEN=8192
TOKENIZER_MODE="mistral"
# Adjust based on desired response length
MAX_TOKENS=512
# Optional: Specify tensor_parallel_size if using multiple GPUs
# TENSOR_PARALLEL_SIZE=1

# TTS Configuration - Currently Disabled because of unidentified Bug
TTS_MODEL="tts_models/en/ljspeech/vits" # Or another coqui-ai/TTS compatible model
ENABLE_GPU_TTS=false # Set to true if you have enough GPU VRAM for TTS

# Whisper Configuration (Optional - if voice input is processed backend-side)
# WHISPER_MODEL="large-v3-turbo"

# Static File Paths (Relative to backend/app directory)
STATIC_DIR="static"
UPLOAD_DIR="static/uploads"
AUDIO_DIR="static/audio"
